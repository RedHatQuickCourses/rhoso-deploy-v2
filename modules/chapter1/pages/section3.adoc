= DELETE Preparing RHOCP for RHOSP Network Isolation

Network confiuration for the nodes is facilated by the below operators.

*NMState operator* _facilitates network configuration for the nodes of the cluster._
`NodeNetworkConfigurationPolicy` offered by nmstate operator allows you to configure network settings on individual nodes in a network. 
This policy can be used to set network interfaces, routing tables, firewall rules, and other network-related settings. 

*Multus CNI plugin* _facilitates the network configuration for the pods running control plane services for the cluster._
`NetworkAttachmentDefinition` (NAD) is a feature offered by the Multus CNI (Container Network Interface) plugin. 
NAD is used to define network attachment points and states for network devices, allowing for automatic fencing and management of those devices. 

*MetalLB operator* _offers load blaancing service for OCP baremetal cluster._
`IPAddressPool` and `L2Advertisement` CRs offered by MetalLB operator facilitates IP advertisement from specified pools, ensuring efficient IP allocation to services and namespaces. 
By leveraging these custom resources, OpenShift facilitates efficient communication between nodes within the cluster, promoting seamless integration and performance optimization in networking-intensive environments.

FIXME: review the explanation above.

Please refer to the following table for the network configuration utilized in this deployment:

FIXME: review/change this table if required. also see if any google spreadsheet link can be provided for such configuration in the custom deployment.

[cols="1,1,1,1,1,1,1"]
|===
|Network name | VLAN | CIDR	| NetConfig allocation | Range	MetalLB IPAddressPool range	| nad ipam range | OCP worker nncp range

| ctlplane
| n/a
| 172.22.0.0/24
| 172.22.0.100 - 172.22.0.120
| 172.22.0.80-172.22.0.90
| 172.22.0.30 - 172.22.0.70
| 172.22.0.10 - 172.22.0.20

| internalapi
| 20
| 172.17.0.0/24
| 172.17.0.100 - 172.17.0.250
| 172.17.0.80-172.17.0.90
| 172.17.0.30 - 172.17.0.70
| 172.17.0.10 - 172.17.0.20

| storage
| 21
| 172.18.0.0/24
| 172.18.0.100 - 172.18.0.250
| 172.18.0.80-172.18.0.90
| 172.18.0.30 - 172.18.0.70
| 172.18.0.10 - 172.18.0.20

| tenant
| 22
| 172.19.0.0/24
| 172.19.0.100 - 172.19.0.250
| 172.19.0.80-172.19.0.90
| 172.19.0.30 - 172.19.0.70
| 172.19.0.10 - 172.19.0.20

| external
| n/a
| 192.168.123.0/24
| 192.168.123.61 - 192.168.123.90
| 192.168.122.80 - 192.168.122.90
| 192.168.123.60 - 192.168.123.110
|  

|===


We will create the required configuration files from scratch by referring to the sample configuration provided in the product documentation:

https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/18.0-dev-preview/html-single/deploying_red_hat_openstack_platform_18.0_development_preview_3_on_red_hat_openshift_container_platform/index#doc-wrapper[Deploying Red Hat OpenStack Platform 18.0 Development Preview 3 on Red Hat OpenShift Container Platform]

Sample configuration files captured from the product documentation are provided, named with the prefix `sample-*`. 
Additionally, preconfigured YAML files, starting with `osp-ng-*`, are included for your deployment in the lab.

**IMPORTANT:** The pre-configured YAML files are included for your convenience. 
However, we recommend creating your own YAML files using the sample configuration from the product documentation. 
You may use the provided sample configuration in the lab to avoid copy-pasting from the documentation and refer to the pre-configured files for reference as needed.

**NOTE:** For deployments in other environments, consider creating a table listing the necessary network isolation and their corresponding IP/subnet ranges. 
This will help streamline the process.

. Clone the Files Repo. FIXME: Provide steps to download the config file zip 
+
In the bastion terminal, clone the repo and change directory to the files that we will used later in the lab
+
[source,bash]
----
git clone https://github.com/pnavarro/showroom_osp-on-ocp-lb1374.git labrepo
cd labrepo/content/files
----

NOTE: For the following steps, we are referring to https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/18.0-dev-preview/html-single/deploying_red_hat_openstack_platform_18.0_development_preview_3_on_red_hat_openshift_container_platform/index#proc_preparing-RHOCP-for-RHOSP-network-isolation_preparing[Section 3.4. Preparing RHOCP for RHOSP network isolation] from Deploying Red Hat OpenStack Platform 18.0 Development Preview 3 on Red Hat OpenShift Container Platform guide.

. Create required projects for the Openstack Operator installation.
+
The next installation step for the *OpenStack Operators* involves creating the *openstack-operators* and *openstack* projects for the RHOSO operators.
+
[source,bash]
----
oc new-project openstack-operators
oc new-project openstack
----




. get list of nodes in your ocp cluster.
+
Worker nodes:
+
[source,bash]
----
oc get nodes -l node-role.kubernetes.io/worker -o jsonpath="{.items[*].metadata.name}"
----
+
.Sample output
----
ocp4-worker1.aio.example.com ocp4-worker2.aio.example.com ocp4-worker3.aio.example.com
----
+
Master nodes:
+
[source,bash]
----
oc get nodes -l node-role.kubernetes.io/master -o jsonpath="{.items[*].metadata.name}"
----
+
.Sample output
----
ocp4-master1.aio.example.com ocp4-master2.aio.example.com ocp4-master3.aio.example.com
----



. copy sample nncp configuration from (refer step #4).
+
or use the provided sample configuration file `single-nic-vlan-nncp.sample`.

. Create a copy of sample config for one of your nodes
[source,bash]
----
cp nncp.sample osp-ng-nncp-master1.yaml
----

. Edit the file for use with the relevant node. Refer to the network configuration table you have created earlier.
[source,bash]
----
vi osp-ng-nncp-master1.yaml
----

. Customize the copy of sample configuration as per requirement.
.. Change the metadata name to reflect the node name and interface name strings.
.. Change the name of the interface as per the network configuration table.
.. Change the vlan id as per your setup.
.. Change the ipaddress of the interface as per the network configuration table.
.. Repeat above steps for three network configuration blocks: internalapi, storage and tenant.
.. Change the hostname under nodeselector to match your hostname.
.. Change the node-role string if required.

. Apply the configuration
[source,bash]
----
oc apply -f  osp-ng-nncp-master1.yaml
----

. Verify the network settings are applied on the relevant node by listing ip settings on the node.

.. Connect to node:
[source,bash]
----
oc debug node/ocp4-master1.aio.example.com
----

.. Check IP address listing to verify the desired settings are applied.
[source,bash]
----
sh-4.4# ip address show
----
+
.Sample output
----
. . . 
66: enp1s0.20@enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether de:ad:be:ef:00:01 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.101/24 brd 172.17.0.255 scope global noprefixroute enp1s0.20
       valid_lft forever preferred_lft forever
67: enp1s0.21@enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether de:ad:be:ef:00:01 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.101/24 brd 172.18.0.255 scope global noprefixroute enp1s0.21
       valid_lft forever preferred_lft forever
68: enp1s0.22@enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether de:ad:be:ef:00:01 brd ff:ff:ff:ff:ff:ff
    inet 172.19.0.101/24 brd 172.19.0.255 scope global noprefixroute enp1s0.22
. . . 
----

. Create copy of your current edited file for other nodes, it will be easier to edit.
[source,bash]
----
cp osp-ng-nncp-master1.yaml osp-ng-nncp-master2.yaml 
cp osp-ng-nncp-master1.yaml osp-ng-nncp-master3.yaml 
cp osp-ng-nncp-master1.yaml osp-ng-nncp-worker1.yaml
cp osp-ng-nncp-master1.yaml osp-ng-nncp-worker2.yaml
cp osp-ng-nncp-master1.yaml osp-ng-nncp-worker3.yaml
----

. Edit the other files with relevant network configuration as per network configuration table.
. Most likely you will need to change only the metadata name, the ip addresses, nodeselector hostname and node-role configuration.

Copy sample file for the network-attachment-defination configuration from the same documentation url (step #8)

Change the interface name and ipaddress as per your setup. Refer to network cofiguration table.
Copy the ctlplane block and paste it at the bottom and change it for external network configuration with the relevant interface and ip addresses.

Apply config for NAD

Copy sample file for the MetalLB IP address pools configuration from the same documentation url (step #12)
Make sure ip address ranges are as pre desired configuration form the network configuration table.
Apply the configuration in osp-ng-metal-lb-ip-address-pools.yaml file


Copy sample L2Advertisement configuration for MetalLB from the same documentation url (step #16)
Change the configuration as per your setup. You may just need to chance the interface names as per the network configuration table.










. Apply preconfigured yamls indivdually:
+
[source,bash,role=execute]
----
oc apply -f osp-ng-nncp-w1.yaml
oc apply -f osp-ng-nncp-w2.yaml
oc apply -f osp-ng-nncp-w3.yaml
oc apply -f osp-ng-nncp-m1.yaml
oc apply -f osp-ng-nncp-m2.yaml
oc apply -f osp-ng-nncp-m3.yaml
----

. Wait until they are in an available state before proceeding:
+
[source,bash,role=execute]
----
oc get nncp -w
----
+
.Sample Output
[source,bash]
----
NAME                              STATUS      REASON
osp-enp1s0-master1-ocp4-master1   Available   SuccessfullyConfigured
osp-enp1s0-master2-ocp4-master2   Available   SuccessfullyConfigured
osp-enp1s0-master3-ocp4-master3   Available   SuccessfullyConfigured
osp-enp1s0-worker-ocp4-worker1    Available   SuccessfullyConfigured
osp-enp1s0-worker-ocp4-worker2    Available   SuccessfullyConfigured
osp-enp1s0-worker-ocp4-worker3    Available   SuccessfullyConfigured
----

. Before proceeding configure a *nad* resource for each isolated network to attach a service pod to the network:
+
[source,bash,role=execute]
----
oc apply -f osp-ng-netattach.yaml
----

. Verify the `network-attachment-definitions` created with the above command:
+
[source,bash,role=execute]
----
oc get network-attachment-definitions.k8s.cni.cncf.io -n openstack
----
+
.Sample Output
----
NAME          AGE
ctlplane      5s
external      5s
internalapi   5s
storage       5s
tenant        5s
----

. Once the nodes are available and attached (FIXME elaborate) configure the *MetalLB IP address range* using a preconfigured yaml file:
+
[source,bash,role=execute]
----
oc apply -f osp-ng-metal-lb-ip-address-pools.yaml
----
FIXME: what is the difference in the range mentioned in NAD whereabbouts v/s metallb ipaddresspools

. Verify the metallb ip address pools using below command:
+
[source,bash,role=execute]
----
oc get ipaddresspools.metallb.io -n metallb-system 
----
+
.Sample Output
----
NAME          AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES
ctlplane      true          false             ["172.22.0.80-172.22.0.90"]
internalapi   true          false             ["172.17.0.80-172.17.0.90"]
storage       true          false             ["172.18.0.80-172.18.0.90"]
tenant        true          false             ["172.19.0.80-172.19.0.90"]
----

. Lastly, configure a *L2Advertisement* resource which will define which node advertises a service to the local network which has been preconfigured for your lab environment:
+
[source,bash,role=execute]
----
oc apply -f osp-ng-metal-lb-l2-advertisements.yaml
----

. Verify the configuration:
+
[source,bash,role=execute]
----
oc get l2advertisements.metallb.io -n metallb-system 
----
+
.Sample output
----
NAME          IPADDRESSPOOLS    IPADDRESSPOOL SELECTORS   INTERFACES
ctlplane      ["ctlplane"]                                ["enp1s0"]
internalapi   ["internalapi"]                             ["enp1s0.20"]
storage       ["storage"]                                 ["enp1s0.21"]
tenant        ["tenant"]                                  ["enp1s0.22"]
----

FIXME: Any explanation about the significance of these?
